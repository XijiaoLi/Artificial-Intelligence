{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalB_release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjnUTgMYK-b9"
      },
      "source": [
        "# Problem 1 (12 points)\n",
        "\n",
        "Recall the word ladder puzzle from Homework 1, in which vowel changes cost twice those of consonant changes. You implemented best-first search as a general procedure for solving this problem. Suppose now that we do not maintain a ```reached``` table and that children of expanded nodes are always added to the frontier.\n",
        "\n",
        "1. Explain how this change affects the property of completeness. Remember that it is possible for a problem to have no solution. Consider each of the following algorithms: depth-first search, breadth-first search, uniform-cost search, and A* search with the simple Hamming distance heuristic.\n",
        "\n",
        "2. Suppose that multiple solutions exist for a given problem. Without a reached table, which of the above four algorithms are guaranteed to return an optimal solution, and which are not? Explain for each of them.\n",
        "\n",
        "3. Explain whether the \"simple\" Hamming distance and the Hamming distance with vowels heuristics are still admissible when A* is implemented without a reached table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwHNvjNMaM1"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1. A searching algorithm is complete if the search tree is finite. All these four algotirhms are complete in the original setting. If we do not maintain a `reached` table, the only change is that there will be some loop in our searching route, i.e., some path in the search tree can be something like \"let\" -> \"get\" -> \"let\".\n",
        "   - depth-first search: not complete, since the search tree will be infinite due to the paths corresponding to a loop.\n",
        "   - breadth-first search: complete, if it has a solution, since if the shallowest goal node is at depth `d`, it will eventually find it after expanding all the nodes shallower than `d`, with a finite braching factor. Even if there exist some looping path, instead of continuing down to exhaust its depth, we first search through some other (correct and non-looping) paths.\n",
        "   - uniform-cost search: complete, if it has a solution. Similar as BFS, if there is a goal node with the least finite cost `c`, UCS will eventually find it after expanding all the nodes with cost lower than `c`, which is also finite since we always increase the `cost` of the new state to make it higher then that of the old node, and, we have a finite branching factor.\n",
        "   - A* search with the simple Hamming distance heuristic: complete, if it has a solution. Similar as BFS and UCS, except that here the `cost` includes the simple Hamming distance heuristic.\n",
        "\n",
        "2. - depth-first search: not optimal, since the search tree will be infinite due to the paths corresponding to a loop.\n",
        "   - breadth-first search: not optimal. BFS is optimal if all operators (i.e., arcs) have the same constant cost, or costs are positive, non-decreasing with depth. However, the rule that vowel changes cost twice those of consonant changes will violate this premise.\n",
        "   - uniform-cost search: optimal. Unlike BFS which expends by depth, UCS explores increasing cost contours, so the first encountered solution must also be the optima.\n",
        "   - A* search with the simple Hamming distance heuristic: optimal. A* is optimal if the heuristic is admissible; in our case, the Hamming distance heuristic is indeed admissible.\n",
        "\n",
        "3. Both are still admissible, since when A* is implemented without a reached table, the cost of the optimal solution will not be changed. If there is  ùë•  characters diffrent between the current state and the goal, we need to change at least  ùë•  characters to achieve the solution, so cost of the optimal solution is at least  ùë• "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kYyNrCNC7kJ"
      },
      "source": [
        "# Problem 2 (16 points)\r\n",
        "\r\n",
        "We are computing the value $v_0$ of a chance node $C_0$ in a game tree. $C_0$ has children $C_1, C_2, ..., C_n$, occurring with probability $p_1, p_2, ..., p_n$, respectively. We have already retrieved the values of the first $k$ children: $v_1, ..., v_k$. We have not yet seen the values of the remaining children nodes. Give an expression for the maximum possible value $v_0$, if the maximum possible value for any child node is $v_{\\text{max}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5uqMxKoD-HA"
      },
      "source": [
        "ENTER YOUR EXPRESSION HERE\r\n",
        "\r\n",
        "For chance nodes, node value is the expected value over all its children. \\\\\r\n",
        "$v_{0\\ max} = \\sum_{i=k}^{n}p_{i}v_{max} + \\sum_{i=1}^{k}p_{i}v_i$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osyq0--xMqjV"
      },
      "source": [
        "X and O are playing 3x3 tic-tac-toe. Instead of playing optimally, O now plays randomly, choosing its move according to known probabilities each turn. X's strategy is implemented in an ```X_value``` function (signature shown below). You also have access to functions returning O's ```actions``` in a given state and the ```result``` of taking an action from a given state.\n",
        "\n",
        "```\n",
        "def X_value(state):\n",
        "  INPUTS: A game state\n",
        "  OUTPUT: Value of state assuming it is X's turn to move\n",
        "\n",
        "def actions(state):\n",
        "  INPUT: A game state during O's turn\n",
        "  OUTPUTS: List of (action, probability) tuples\n",
        "\n",
        "def result(state, action):\n",
        "  INPUTS: A game state and action\n",
        "  OUTPUT: New game state as a result of O taking action from state\n",
        "```\n",
        "\n",
        "Calling the above functions as necessary, complete the ```O_value``` function below, which returns the value of the given game state assuming it is O's turn to move. As the function loops through the children states to compute the expected value, it should also prune when possible. ```O_value``` should stop retrieving the values of its remaining children nodes as soon as it recognizes that it is no longer possible to return a value greater than ```alpha``` (it can simply return the value computed so far).\n",
        "\n",
        "As a hint, you should be using the expression you derived above. You may assume that ```V_MAX``` is a known upper bound on state utility values and accessible in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AToluZq8PPHR"
      },
      "source": [
        "def O_value(state, alpha):\n",
        "  isTerminal, score = terminal(state)\n",
        "  if isTerminal:\n",
        "    return score\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  v = V_MAX\n",
        "  acts = actions(state)\n",
        "  for a, p in acts:\n",
        "    new_state = result(state, a)\n",
        "    v2 = X_value(new_state)\n",
        "    v -= (V_MAX-v2)*p\n",
        "    if v <= alpha:\n",
        "      return v\n",
        "  return v\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF6oNrSzsbjE"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pYqZlMfs-Sl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "038b9188-5455-4eae-82ba-98011dccfe6a"
      },
      "source": [
        "def terminal(s):\n",
        "  return False, None\n",
        "def actions(s):\n",
        "  return [('a1',0.5), ('a2',0.5)]\n",
        "def result(s,a):\n",
        "  if a == 'a1': return 's1'\n",
        "  elif a == 'a2': return 's2'\n",
        "def X_value(s):\n",
        "  if s == 's1': return 1\n",
        "  elif s == 's2': return 1\n",
        "\n",
        "V_MAX = 1\n",
        "print(O_value('s0',1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhuewOeyQp4K"
      },
      "source": [
        "# Problem 3 (20 points)\n",
        "\n",
        "Recall that $TD(0)$ solves the prediction problem of evaluating a fixed policy $\\pi$ using temporal-difference updates to estimated state values. The update to the value $V^\\pi(s)$ is given by\n",
        "\n",
        "$$ V^\\pi(s) \\leftarrow V^\\pi(s) + \\alpha (r + \\gamma V^\\pi(s') - V^\\pi(s)), $$\n",
        "\n",
        "where $r$ is the observed reward, $s'$ is the successor state, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor.\n",
        "\n",
        "Suppose that our agent has traversed a sequence of states $s_1, s_2, ..., s_n$ (following $\\pi$) and observed the corresponding rewards $r_1, r_2, ..., r_{n-1}$. All distinct states in the problem have been encountered at least once prior to $s_n$. The underlying transition and reward functions map $(s,a,s')$ inputs to fixed probabilities and real values, respectively. \n",
        "\n",
        "1. Provide a set of equations in terms of the observed states, rewards, and problem parameters that, if solvable, produces the values to all states $V^\\pi(s)$ such that all TD updates using the observed state-reward sequences are zero. It is acceptable for your set to contain possibly redundant equations.\n",
        "\n",
        "2. How many unknowns are there in your equations? Explain whether it is possible to have more *unique* equations than unknowns and why that may occur.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEXBTZgQB_gJ"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\r\n",
        "\r\n",
        "1. $r_{1} + \\gamma V^\\pi(s_{2}) = V^\\pi(s_{1})$  \r\n",
        "\r\n",
        "   $r_{2} + \\gamma V^\\pi(s_{3}) = V^\\pi(s_{2})$ \r\n",
        "\r\n",
        "   ... \r\n",
        "\r\n",
        "   $r_{n-1} + \\gamma V^\\pi(s_{n}) = V^\\pi(s_{n-1})$\r\n",
        "\r\n",
        "2. n unknowns are there in my equations: $s_1, \\dots, s_{n}$ if $ \\gamma$ is also given; otherwisse, n+1. it is possible to have more unique equations than unknowns, since there might be duplicate states among $s_1, \\dots, s_{n}$, e.g., there are only three distinct states $A, B, C$, and $s_1 = s_{3} = A $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTKfM0MVLFWL"
      },
      "source": [
        "As with policy evaluation using a known model, an alternative to solving the equations that you wrote is dynamic programming, this time using the TD update rather than Bellman update. Starting with an initialized dictionary of values ```V``` of the form ```{state:value}```, this scheme should sweep over the lists of observed ```states``` and ```rewards``` sequences, performing a TD update to  ```V``` for each state-reward pair seen. This sweep should then be repeated until the maximum absolute change for a value is smaller than the provided ```threshold```. (You may assume that the provided inputs will produce a solution that converges.)\r\n",
        "\r\n",
        "Implement the described algorithm in ```TD0``` below. In addition to the described inputs, note that we also have the ```alpha``` and ```gamma``` parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPsJniMuDfIT"
      },
      "source": [
        "def TD0(V, states, rewards, alpha, gamma, threshold):\n",
        "  max_diff = float(\"inf\")\n",
        "  while max_diff >= threshold:\n",
        "    # YOUR CODE HERE    \n",
        "    max_diff = 0\n",
        "    for i in range(len(rewards)):\n",
        "       delta = alpha*(rewards[i] + gamma*V[states[i+1]] - V[states[i]]) \n",
        "       V[states[i]] += delta\n",
        "       if delta > max_diff:\n",
        "         max_diff = delta\n",
        "\n",
        "  return V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CnvxuufwsTS"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz1isalpwsTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd28ce82-2fe5-4861-d5dc-e58df2ec95d5"
      },
      "source": [
        "states = ['s1', 's2', 's1']\n",
        "rewards = [1, 1]\n",
        "alpha = 0.5\n",
        "gamma = 0.8\n",
        "threshold = 1e-6\n",
        "\n",
        "V = {'s1':0, 's2':0}\n",
        "print(TD0(V, states, rewards, alpha, gamma, threshold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'s1': 4.9999935821469315, 's2': 4.999994000289807}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuCoInshppF6"
      },
      "source": [
        "# Problem 4 (16 points)\n",
        "\n",
        "We have a Naive Bayes model with class variable $Y$ and feature variables $F_1, ..., F_n$. Suppose we observe feature $F_e = f_e$. \n",
        "\n",
        "1.  Give an expression for $\\Pr(F_q \\mid f_e)$, the distribution of the query feature $F_q$ given the evidence feature $f_e$. You may also provide your answer in the form of an unnormalized distribution. All quantities in your expression should be contained in the set of Naive Bayes parameters.\n",
        "\n",
        "2.  Briefly describe how you would estimate $\\Pr(F_q \\mid f_e)$ using likelihood weighting instead. What variables should be sampled, what order are they sampled in, and what sampling distributions are used? Also be sure to describe how the desired distribution is obtained from samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YULf95khAEnN"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\r\n",
        "\r\n",
        "1.  $\\Pr(F_q \\mid f_e) = \\frac{\\sum_y\\Pr(F_q, y, f_e)}{\\Pr(f_e)} = \\frac{\\sum_y\\Pr(F_q | y) \\Pr(y , f_e) }{\\Pr(f_e)} = \\frac{\\sum_y\\Pr(F_q | y) \\Pr(f_e |y) \\Pr(y) }{\\Pr(f_e)}$\r\n",
        "\r\n",
        "2.  Fix evidence variables $F_e = f_e$, sample only nonevidence variable $Y, F_q$ ($Y$ first), and weight each sample by the likelihood it accords the evidence. Sampling distributions: \\\\\r\n",
        "$ S_{WE}(F_q, f_e)w(F_q, f_e) = Pr(F_q | y) \\Pr(f_e |y)$ \\\\\r\n",
        "To extract the distribution of the query variable, the sum of the weights of the samples where $F_q = f_q$ for each of $F_q$'s possible value, and then do a normalization at last."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObUO85S9Acc-"
      },
      "source": [
        "Use the expression that you came up with in 1. to implement ```feature_likelihood``` below. The function inputs are as follows:\r\n",
        "*   ```prior``` is a 1D numpy array containing the distribution $\\Pr(Y)$.\r\n",
        "*   ```fe_given_y``` is a 1D numpy array containing the probabilities $\\Pr(f_e \\mid Y)$.\r\n",
        "*   ```fq_given_y``` is a 2D numpy array, in which ```fq_given_y[i,j]``` is the probability $\\Pr(F_q = i \\mid Y = j)$.\r\n",
        "\r\n",
        "Make sure that your outputs are properly normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmispu48d1m"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def feature_likelihood(prior, fe_given_y, fq_given_y):\n",
        "  # YOUR CODE HERE\n",
        "  len_q, len_y = fq_given_y.shape\n",
        "  ret = np.zeros(len_q)\n",
        "  for i in range(len_y):\n",
        "    ret += fq_given_y[:,i]*fe_given_y[i]*prior[i]\n",
        "  \n",
        "  return np.divide(ret, np.sum(ret))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KFErZRABW2"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJoAKIgk-39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e59704-fb85-4862-8a32-f056164892e6"
      },
      "source": [
        "prior = np.array([0.5, 0.5])\n",
        "fe_given_y = np.array([0.7, 0.6])\n",
        "fq_given_y = np.array([[0.3, 0.6], [0, 0.4], [0.7, 0]])\n",
        "\n",
        "print(feature_likelihood(prior, fe_given_y, fq_given_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.43846154 0.18461538 0.37692308]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}