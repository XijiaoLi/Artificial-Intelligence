{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Xijiao Li - hw4b_release.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkhCnnA-32fP"
      },
      "source": [
        "# Problem 1 (40 points)\n",
        "\n",
        "A robot is wandering around a room with some obstacles, labeled as $\\#$ in the grid below. It can occupy any of the free cells labeled with a letter, but we are uncertain about its true location and thus keep a belief distribution over its current location. At each timestep the robot moves from its current cell in one of the four cardinal directions with uniform probability, and moving toward either a wall or a $\\#$ cell will cause the robot to stay in its current cell. For example, starting from A the robot can either stay in A with probability $\\frac12$ or move to B or C with probability $\\frac14$ each.\n",
        "\n",
        "![Picture1.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASEAAAEzCAMAAABJ3UW5AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAADwUExURQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAUFBQYGBgcHBwgICAkJCQoKCgsLCxAQEBERERMTExYWFhgYGBsbGxwcHB8fHyEhISMjIycnJywsLC8vLzQ0NEREREdHR0pKSlJSUlVVVVdXV1paWl9fX2ZmZmhoaGlpaXNzc3d3d3p6eoKCgoODg4WFhYmJiY6OjpKSkpWVlZiYmJycnKampqenp6qqqrKysrW1tby8vMPDw9DQ0NXV1d3d3ezs7O7u7v///zN1yboAAAAWdFJOUwAqP1tud4COk5Wen6S6vsXIzM7W2uwjB0enAAAACXBIWXMAABcRAAAXEQHKJvM/AAAMh0lEQVR4Xu2da2PrOBGGD/f7HcqdsMBSoAQol7AFym4gUBoa+v//DZqRlFiN43dm4lAd9n2+VEkd2XkkjSTHlt8QQgghhHzo+Xri85+9EF9OmX+1pGfnc3LoJT0/X0uZf1oEffyKnOQbNASgIQQNIdTQRyX1l/9ciJ+nzN8t6dl5Xw797+XF7HwvZf6FvaG/Pl+IX6TMf1rSs/M3OfR/lhez8/2UOQ1NQUMIGkLQEIKGEDSEoCEEDSFoCEFDCBpC0BCChhA0hKAhBA0haAhBQwgaQtAQgoYQNISgIUTQ0OY6bb24K68wNkN3cgyV5e36qbwPcBi6T1vel/RT+g7XJT1B0NBCtr66Mn4Fo6FtznTA8rH8axKHoWXaspbrg3yspCeIGdLME7U4ICZDjznTBsseHIakZGuWUp8WJT1BzNBKNk4YKmkmbMiiyGFItnwoaWnTy5KeIGRoJ9sq2/IOwm7o5jGzXpWWjHdhN6TtuLZcaXG3JT1ByNBGttWwuirvIOyGDqW6u5XXhl3YDekeSvr5JqUNNTRkSOzfPMkHDO1YiRjK+zEEU7uh9TA/Sa9LeoKIIXWz1jLYN2pAzJC+sW8VJ7EbkuC834Ep75ghLYmn/MfQkIWYIf0ScxqSdlt3oHszxNGIIak8NzVe78qb0wQNaTOD1dRuSPKroUf3VtJTBAxphyANWCPpJr8JOMfQPHVos0xI93gtiYTMCq7SX9QKAoa0E5PRtHZpzTc6yTmtDA7cTYZy1B8DlEDAkMjX75GbmWnmETOkQ3c8Ku3NkB52blvazAwdZtSQfiucv8lQMysesgCR1G9IZxw5V2MZJyKGdrqn1CUgzJFaet/9CE5ikqV03YZ2knONbjoxsMw8/Ia2aw2lN4a+0mwoMhzyG9LwXDswLWXLWSK7oZZby2DCbEiiQi1c3ZmlcN2GNPbU49ZmZpl5BA0t1rMaigyH3IZ0xnEYQmgzM8w8onXoajHn2Y9h6DGeHfIb0qnGYZSozcwwwQ8bSoEDViOzIdmuhh7j2SG/IZlxDPrHfOIUNwV/pH5+3JQhDJz6WQ3pwdbQI5nDnAWnId3JsM5oh4NnHgFDiUfNHWZvNdSEHuPZIbchHXcN446+gWtrzFAeWsARl9VQMxySz5gGu05DUqhNfMvNDM48goby2AJ1BQZDu9syV13orDUhn7lJf2GH7zNUf+N4CSyMqKHcikFrMBjKpseAschnqP7G8RI48wgb0uEXaMUGQ9K3jwPydhrKYWEMVFfDhvSrnW8oB4MxYC/jMnS6rqKZx2sbEvRkjXs45DM0elJRezM0Og0b0nY9jyHNvvYpEqltP2V5DOmM42h4mN8F3c1rR2pBu5mStg+HXIZO/LihvwqB8ogayu0anKQwGpIGu+9S5CN4oCt4DKmK48qi4sDMI2hoO+eIUQzV3JuYNI3DkPYHIwEnN7PpAokZesx950yzjuFMrIlJ0zgMaUgea02GMYvdUL2y4fHhXuvsfDNXMVRDj+6qpAEOQxo0xyJyDhaTJWI3dAQ+D2s0JFsNzw6ZTrB7DOnxj/bq+VehyZlH3NAdPrXiMFRDzzAmAeyGJk6WaTOb/FHCZGhk4Gu6TM9mSP27zw55DGnFHJ9dbKUBThaJyVCOdHvMl3raDEm/uG9YUty2zt4Th87BZiiIsZUFoSEEDSFoCEFDCBpC0BCChhA0hKAhBA0haAhBQwgaQtAQgoYQNISgIQQNIWgIQUOI1tAv7y7EOynzH5T07PxKDv035cXsfDNl/hUxxGctTMCnUSBoCEFDiMMTTd7794X4Wcr8JyU9O3+SQ/+gvJid76bM2dtPwfEQgoYQNISgIQQNIWgIQUMIGkLQEIKGEDSEoCEEDSFoCEFDCBpC0BCChhA0hKAhBA0haAgRNbTbrPIiR6uN5VbLD52hp2Yholt8u67B0E6NV5bLe7CUyAGroc3I4kB4uc2QIbkpuAEuvWkwlBfHaFjZ1p2wGhpbPQnfdB8wtCtrKQxZgKZmMHSkXVhamrDVkOb4ArQsUMRQFbRYbR4Pj9UAN69HDV0tDPd8n2MI33TvNlQEHY58p09ZAotoWA2t8pofj+u7fUXF6yi5DJUdVPCKEG5DedWAZjGOnbxX0iewGhosmvR0X+IGXEjJZaik7XgN5bU5Xq5W8rAAC20HDCXzuloGXB+rM0PaJR/H5aeZIvWL+pIHFWid9q4M5SpkWi20IWioKALtrCtDWu+NC/cMiRraaS8AKlFPhvJKQ6BIx4gaKqs/TXf5PRkyrFY1TthQXjlrejGlngxpWAg0sjMM5dFFeTFOT4a0QG1LYbbEDeW+YXKC1pMh2SbQk51jyLDPjgwZyvMEZxjS6cfI+wdchspsI4PnHG+DIR2jTrZsl6EGS8AIGCovXJxhCK+HGTdkWXDQZUi/xP/akL5PQ4lThrS7v5ChyfBW6N/QzHGopO0E4lBgSH22ocnC7s7Q293bl7SdgCHzrzQDzjAkb79lY+rJ8jxB3JChZfdkSOdlxjV5G+KG9O3pyXJPhnT0hn9iOiZuyDBZ7slQPj8UCERhQ5bQ15OhfI7RMhJ9QdiQ9mTgjFRPhnIzC/T3UUP6JlpruytDudKP/DgGHq0UNJRbNXrobVeG8gj3KHJub8CeY4bKqvDowVZ9GSqr/a+ac09yYUtJniBgqFwPYFjPvi9D9TKf60P3cpErG+5zZU3giz9chsoOKniS6TZUfgZNju6GV8eA7sZq6IiFYWjhMvSCS1w/tFc0ZDFTpH5J25pPcI4h/GjpgKHnh6PL3S5zld71ne1Ei9XQ2FV6+FxOxNDzrl7akzFcbmgw1Fzpeb1crWHpVqyGxq70xI/PDxlKPNzp41GvlquNpSEYDMWxGooRNeSEhhA0hKAhBA0haAhBQwgaQtAQgoYQNISgIQQNIWgIQUMIGkLQEIKGEDSEoCHE/42hX79/IX6cMv9hSc/Ob+XQf19ezM63UuZfEkN81sIEfBoFgoYQNIQ4PNFk9a8LIb/P/qikZ+ePcuh/Li9m5zspc/b2U3A8hKAhBA0haAhBQwgaQtAQgoYQNISgIQQNIWgIQUMIGkLQEIKGEDSEoCEEDSFoCEFDCBpC0BAiYkjuBN6vjrLEN0onrIa2ZRWCAfhOVIchuSG7rnrwlPZlWAU4Ykh2U49bV5OZ5U5gpayM3wBzdxiSu43rY14e5GMlPUHEkHyNWod0qZT56tDwfunKnIbkbulah6SgDatNRQwFCuIMQ+g5Ai5DsmVdBULWXTEspRQxFCgIl6F1XpKjgFclsBvSdV1qnZR9GdZ0ixiSLetujAXhMoTDWovdkMaEktblsdrVfEYJGIoURCeGdOWMktaCNqxGHjDUFIR8JUNBdGJIYsK+xht3FTAUKYhODAV64YihSEF0YmhY45umMIHP0HaZkGHvQhIJXQzwJiVAj/P6hjZyuNILX0sikRd2WS5RFPUZkuozDohFr29Icx4F7I6G5jWkI+hRwG5e31BZufAYNGb3R2odDtU+QDoHy1rDfURq6YX3EwCJSZaHs/gNRYZDnRiK9MIBQ01ByIcsBeEy1M7Lyn8mMBsa1njrcChgKFQQLkMteEFGs6FhjbcOhwKGQgVxhiHciM2GhqFHCtpwUiJiaFgQxrNDvRiS7WqNN56UiBgKFUTcEHoec8JqqOmFZU+WXjhgSLZzF4TLkCWwDbEaakKP8exQwFCsILowFOqFfYbu0kRP56o69UvUmSBcIfm1De1u01EOp9y6I5lyw37GY+hJNhkFxqLXNpSXkx8DNgGPIW3H45QtTvLahk5PuWEYdbUyaVSjwKftvbYhjZ6jwADhjtTDPsA8HOoiUjc/D5t7Yb8h2ax6l7pre/5tD4Y0SNTRlezI8pxJv6FgQfRgqKnx5uGQ21CwIHow1NR4+Qh+ioMQMlTSjoLoxVCt8cZLVgSvoWYmJh+xFUQPhiTv5qQEnvEJEUP70CMfsX2dXgzVGt80hWm8hoIF0YMh2Wp4UsLWC4cMBQqiF0M166YpTOM1JFu5zw51YUjL031Swm8ozY/3F3bKdNDW2ZsNyaNSbmwN94DN0DZlvW9Y8mQfWx/jNhTEaiiEsZUFoSEEDSFoCEFDCBpC0BCChhA0hKAhBA0haAhBQwgaQtAQgoYQNISgIQQNIWgIQUOI1tDv/nEh3k2Zv1PSs/MHOfT3yovZ+XbK/ItiiM9amIBPo0DQEIKGEGrozWcSH7sUn0iZf7Kk5+eih/6plPlH1BAhhBBCCCGEEEIIIYQQQgghhJAYb978F4uExitMd6VbAAAAAElFTkSuQmCC)\n",
        "\n",
        "The robot also makes an observation after each transition, returning what it sees in a randomly chosen cardinal direction. Possibilities include observing $\\#$, \"wall\", or \"empty\" (for a free cell). For example, in C the robot observes \"wall\", $\\#$ (each with probability $\\frac14$), or \"empty\" (with probability $\\frac12$).\n",
        "\n",
        "**Note**: You don't have to show work for solving linear equations or eigenvectors, but please show what equations or matrices you use. Feel free to show your work in Python as well. You may also omit computations that will turn out to be zero based on the provided information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwPiioasIfZ"
      },
      "source": [
        "1.  Suppose that the robot wanders around forever without making observations. What is the stationary distribution over the robot's predicted location? \n",
        "\n",
        "2. The initial distribution $X_0$ is uniform over all possible states. The robot makes a transition and observes $e_1 = \\#$. It then makes a second transition and again observes $e_2 = \\#$. What are the belief distributions $\\Pr(X_1 \\mid e_1)$ and $\\Pr(X_2 \\mid e_1, e_2)$?\n",
        "\n",
        "4. Compute the joint distribution $\\Pr(X_1, X_2 \\mid e_1, e_2)$. Hint: First determine the state sequences with a nonzero probability.\n",
        "\n",
        "5. Are the states $X_1$ and $X_2$ independent given $e_1$ and $e_2$? Why or why not? What is the most likely state sequence(s) of $X_1$ and $X_2$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_c8qZSyuGcJ"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1. It is $(\\frac{1}{6},\\frac{1}{6},\\frac{1}{6},\\frac{1}{6},\\frac{1}{6},\\frac{1}{6})$, which is obtained by calculating the normalized eigenvector corresoonding to eigenvalue 1 of the transition matrix. I used Python to do the calculation, and the code is listed below.\n",
        "\n",
        "2. $\\Pr(X_1 \\mid e_1) = (0,\\frac{1}{6},\\frac{1}{6},0,\\frac{1}{3},\\frac{1}{3})$ \\\\\n",
        "$\\Pr(X_2 \\mid e_1, e_2) = (0,\\frac{1}{14},\\frac{1}{14},0,\\frac{3}{7},\\frac{3}{7})$\n",
        "\n",
        "3. $\\Pr(X_1, X_2 \\mid e_1, e_2) = \\Pr(x_1)\\Pr(e_1|x_1)\\Pr(x_2 | x_1) \\Pr(e_2 | x_2) $ \\\\\n",
        "$\\Pr(B, B \\mid e_1, e_2) = \\frac{1}{14}$ \\\\\n",
        "$\\Pr(C, C \\mid e_1, e_2) = \\frac{1}{14}$ \\\\\n",
        "$\\Pr(E, E \\mid e_1, e_2) = \\frac{3}{7}$ \\\\\n",
        "$\\Pr(F, F \\mid e_1, e_2) = \\frac{3}{7}$ \\\\\n",
        "\n",
        "4. $\\Pr(X_2 | e_1, e_2)= (0,\\frac{1}{14},\\frac{1}{14},0,\\frac{3}{7},\\frac{3}{7})$ \\\\\n",
        "$\\Pr(X_2 | x_1=B, e_1, e_2)= (0,0,1,0,0,0)$ \\\\\n",
        "So they are not independent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgTXtGN4Ojtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d55999-151a-4729-8d69-2ffb44502aad"
      },
      "source": [
        "# ENTER ANY CODE FOR THE ABOVE PROBLEM HERE\n",
        "import numpy as np\n",
        "import numpy.linalg as nla\n",
        "\n",
        "#              A    B    C    D    E    F\n",
        "T = np.array([[1/2, 1/4, 1/4, 0  , 0  , 0  ],\n",
        "              [1/4, 1/2, 0  , 1/4, 0  , 0  ],\n",
        "              [1/4, 0  , 1/2, 1/4, 0  , 0  ],\n",
        "              [0  , 1/4, 1/4, 0  , 1/4, 1/4],\n",
        "              [0  , 0  , 0  , 1/4, 3/4, 0  ],\n",
        "              [0  , 0  , 0  , 1/4, 0  , 3/4]])\n",
        "\n",
        "eval, evec = nla.eig(T)\n",
        "print(\"eigenvalues\\n\", eval)\n",
        "print(\"eigenvectors\\n\", evec[:,2])\n",
        "print(\"normalized\\n\", np.divide(evec[:,2], np.sum(evec[:,2])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eigenvalues\n",
            " [-0.30901699  0.25        1.          0.80901699  0.5         0.75      ]\n",
            "eigenvectors\n",
            " [-0.40824829 -0.40824829 -0.40824829 -0.40824829 -0.40824829 -0.40824829]\n",
            "normalized\n",
            " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-j8pYiRxIOe",
        "outputId": "71b14802-453a-48a5-8506-f6a8efd7e0ef"
      },
      "source": [
        "f0 = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
        "#              A    B    C    D    E    F\n",
        "O = np.array([[0  , 0  , 0  , 0  , 0  , 0  ],\n",
        "              [0  , 1/4, 0  , 0  , 0  , 0  ],\n",
        "              [0  , 0  , 1/4, 0  , 0  , 0  ],\n",
        "              [0  , 0  , 0  , 0  , 0  , 0  ],\n",
        "              [0  , 0  , 0  , 0  , 1/2, 0  ],\n",
        "              [0  , 0  , 0  , 0  , 0  , 1/2]])\n",
        "\n",
        "f1_prime = np.matmul(T, f0)\n",
        "f1 = np.matmul(O, f1_prime)\n",
        "print(\"f1\\n\", f1)\n",
        "f1 = np.divide(f1, np.sum(f1))\n",
        "print(\"normalized\\n\", f1)\n",
        "\n",
        "f2_prime = np.matmul(T, f1)\n",
        "f2 = np.matmul(O, f2_prime)\n",
        "print(\"f2\\n\", f2)\n",
        "f2 = np.divide(f2, np.sum(f2))\n",
        "print(\"normalized\\n\", f2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1\n",
            " [0.         0.04166667 0.04166667 0.         0.08333333 0.08333333]\n",
            "normalized\n",
            " [0.         0.16666667 0.16666667 0.         0.33333333 0.33333333]\n",
            "f2\n",
            " [0.         0.02083333 0.02083333 0.         0.125      0.125     ]\n",
            "normalized\n",
            " [0.         0.07142857 0.07142857 0.         0.42857143 0.42857143]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP07RrUTkBBb",
        "outputId": "9f49082d-e935-4652-88e3-e6c0efe720ce"
      },
      "source": [
        "p1_prime = np.matmul(T, f0)\n",
        "p1 = np.matmul(O, f1_prime)\n",
        "print(\"p(x1, e1) =\\n\", p1)\n",
        "\n",
        "p2_prime = np.multiply(T, p1)\n",
        "p2 = np.matmul(O, p2_prime)\n",
        "print(\"p2\")\n",
        "print(p2)\n",
        "p2 = np.divide(p2, np.sum(p2))\n",
        "print(\"normalized\")\n",
        "print(p2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p(x1, e1) =\n",
            " [0.         0.04166667 0.04166667 0.         0.08333333 0.08333333]\n",
            "p2\n",
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.00520833 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.00520833 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.03125    0.        ]\n",
            " [0.         0.         0.         0.         0.         0.03125   ]]\n",
            "normalized\n",
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.07142857 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.07142857 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.42857143 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.42857143]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQMAqpGGUu2g"
      },
      "source": [
        "# POS Tagging\n",
        "\n",
        "In this assignment you will explore [part-of-speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging), a standard task in natural language processing. The goal is to identify parts of speech and related labels for each word in a given corpus. HMMs are well suited for this problem, with parts of speech being hidden states and the words themselves being observations.\n",
        "\n",
        "We will be using data from the English EWT treebank from [Universal Dependencies](https://universaldependencies.org/treebanks/en_ewt/index.html), which uses 17 POS tags. We are providing clean versions of training and test data for you. The data format is such that each line contains a word and associated tag, and an empty lines signifies the end of a sentence. Feel free to open the files in a text editor to get an idea.\n",
        "\n",
        "Start by uploading both files to the Jupyter session storage (you should do this each time that you start a new session). Then run the following code cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8YbOgtrrymZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_sentence(f):\n",
        "  sentence = []\n",
        "  while True:\n",
        "    line = f.readline()\n",
        "    if not line or line == '\\n':\n",
        "      return sentence\n",
        "    line = line.strip()\n",
        "    word, tag = line.split(\"\\t\", 1)\n",
        "    sentence.append((word, tag))\n",
        "\n",
        "def read_corpus(file):\n",
        "  f = open(file, 'r', encoding='utf-8')\n",
        "  sentences = []\n",
        "  while True:\n",
        "    sentence = read_sentence(f)\n",
        "    if sentence == []:\n",
        "      return sentences\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4b10sfi4WC"
      },
      "source": [
        "training = read_corpus('train.upos.tsv')\n",
        "TAGS = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', \n",
        "        'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
        "NUM_TAGS = len(TAGS)\n",
        "\n",
        "alpha = 0.1\n",
        "tag_counts = np.zeros(NUM_TAGS)\n",
        "transition_counts = np.zeros((NUM_TAGS,NUM_TAGS))\n",
        "obs_counts = {}\n",
        "\n",
        "for sent in training:\n",
        "  for i in range(len(sent)):\n",
        "    word = sent[i][0]\n",
        "    pos = TAGS.index(sent[i][1])\n",
        "    tag_counts[pos] += 1\n",
        "    if i < len(sent)-1:\n",
        "      transition_counts[TAGS.index(sent[i+1][1]), pos] += 1\n",
        "    if word not in obs_counts:\n",
        "      obs_counts[word] = np.zeros(NUM_TAGS)\n",
        "    (obs_counts[word])[pos] += 1\n",
        "\n",
        "TPROBS = transition_counts / np.sum(transition_counts, axis=0)\n",
        "OPROBS = {'#UNSEEN': alpha*np.ones(NUM_TAGS) / (tag_counts+alpha)}\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts+alpha)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W-eqo75bsxw"
      },
      "source": [
        "The preceding cell estimates the parameters of the HMM model by going through all the training data and counting each tag, word, and transition that appears. ```TPROBS``` is the transition matrix, in the form of a 2d numpy array. ```OPROBS``` is a dictionary of 1d numpy arrays, with keys as the words appearing in the training data and their values being 1d numpy arrays of the emission probabilities. \n",
        "\n",
        "Notice that we're including one extra \"tag\" in ```OPROBS```: an ```#UNSEEN``` tag. This is necessary because we will inevitably encounter words in the test dataset that we have not seen before. For any word that we have not seen, we treat it as if the word is just ```#UNSEEN```. We assign the \"count\" of an ```#UNSEEN``` word to a constant value $\\alpha$, so that it acts as Laplacian smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfThHKofcPQ"
      },
      "source": [
        "## Coding 1 (5 points)\n",
        "\n",
        "Before we build our HMM, let's try a simplistic unigram model: given a new word, assign it the POS tag giving it the highest emission probability. Context of surrounding words is therefore not considered. This can be done by taking the ```argmax``` of the emission probabilities for the given word in OPROBS. Remember that we treat all words that did not appear before as ```#UNSEEN```. Complete the function below to achieve this (make sure to actually return the POS tag itself, not the tag index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZsS0vfoC7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2486896-4d20-4a71-9722-b3b16d19b137"
      },
      "source": [
        "def unigram(obs):\n",
        "  # Returns the tag of the word obs, as predicted by a unigram model\n",
        "  # YOUR CODE HERE\n",
        "  if not obs in OPROBS:\n",
        "    obs = '#UNSEEN'\n",
        "  i = np.argmax(OPROBS[obs])\n",
        "  return TAGS[i]\n",
        "\n",
        "print(tag_counts)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12479. 17639. 10549. 12353.  6707. 16287.   688. 34779.  3998.  5567.\n",
            " 18577. 12946. 23679.  3841.   599. 23072.   847.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5IuwxZmhGx_"
      },
      "source": [
        "Test out your unigram model by running the cell below, which will tag the specified data and compute accuracy rates over all words and unseen words only. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_28iq5OhHC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7583a2-4a4e-4a60-eccc-4baba9d52d1b"
      },
      "source": [
        "def evaluate(sentences, method):\n",
        "  correct = 0\n",
        "  correct_unseen = 0\n",
        "  num_words = 0\n",
        "  num_unseen_words = 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = [sent[0] for sent in sentence]\n",
        "    pos = [sent[1] for sent in sentence]\n",
        "    unseen = [word not in OPROBS for word in words]\n",
        "    if method == 'unigram':\n",
        "      predict = [unigram(w) for w in words]\n",
        "    elif method == 'viterbi':\n",
        "      predict = viterbi(words)\n",
        "    else:\n",
        "      print(\"invalid method!\")\n",
        "      return\n",
        "\n",
        "    if len(predict) != len(pos):\n",
        "      print(\"incorrect number of predictions\")\n",
        "      return\n",
        "    correct += sum(1 for i,j in zip(pos, predict) if i==j)\n",
        "    correct_unseen += sum(1 for i,j,k in zip(pos, predict, unseen) if i==j and k)\n",
        "    num_words += len(words)\n",
        "    num_unseen_words += sum(unseen)\n",
        "  \n",
        "  print(\"Accuracy rate on all words: \", correct/num_words)\n",
        "  if num_unseen_words > 0:\n",
        "    print(\"Accuracy rate on unseen words: \", correct_unseen/num_unseen_words)\n",
        "\n",
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'unigram')\n",
        "test = read_corpus('test.upos.tsv')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'unigram')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9066014359235022\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8124875483125473\n",
            "Accuracy rate on unseen words:  0.003926701570680628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuG-HJE54TfN"
      },
      "source": [
        "## Response 1 (5 points)\n",
        "\n",
        "You should see that accuracy on the training data is about 90\\%. Accuracy on the test data set is lower at about 81\\%, with accuracy on unseen words only being about 0.4\\%. What does this last number mean? Your answer should be something like \"0.4\\% of [GROUP] have a POS tag of [TAG]\". Briefly explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62lnToxylBbn"
      },
      "source": [
        "ENTER YOUR RESPONSE HERE\n",
        "\n",
        "The meaning is that \"0.4% of unseen words (the words that do not appear in the keya of OPROBS) have a POS tag of SYM\". The reason is that, for each word, we first find its entry in OPROBS to get the list of the probability of it being the certain tag, and then find the tag with the highest probability. If the word is not in OPROBS, we use the entry of '#UNSEEN'. In that list, the tag with the highest probability is the 14th, which is 'SYM'.\n",
        "\n",
        "Since the probability list of '#UNSEEN' is `alpha*np.ones(NUM_TAGS) / (tag_counts+alpha)`, meaning that the more a tag appears, the less probability it maps to. So as the tag that appears most rarely, 'SYM' whereas has the highest probability, which leads to the fact that we predict all the unseen words as 'SYM', the most rare tag, resulting in the low correct rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9mf3c7ymgiN"
      },
      "source": [
        "## Coding 2 (10 points)\n",
        "\n",
        "We will now implement Viterbi to improve our performance over the unigram model. We will split the implementation into several subroutines. First complete the ```elapse_time``` function below. Given a distribution ```m```, it should return an updated \"distribution\" that occurs when applying the Viterbi update in one timestep. In addition, it should also return a list with the *indices* of the most likely prior tag for each current tag.\n",
        "\n",
        "As a hint, these will correspond to ```max``` and ```argmax``` operations, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYnTF9ke87Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e884c218-bafa-4bf2-8773-b05a2e8a6207"
      },
      "source": [
        "def elapse_time(m):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  after a single timestep using Viterbi update, along with a list of the \n",
        "  indices of the most likely prior tag for each current tag\n",
        "  \"\"\"\n",
        "  # mprime = np.zeros(NUM_TAGS)\n",
        "  # prior_tags = np.zeros(NUM_TAGS, dtype=np.int8)\n",
        "  #YOUR CODE HERE\n",
        "  p = np.multiply(m, TPROBS)\n",
        "  prior_tags = np.argmax(p, axis=1)\n",
        "  mprime = np.amax(p, axis=1)\n",
        "\n",
        "  return mprime, prior_tags\n",
        "\n",
        "m0 = np.ones(NUM_TAGS) /NUM_TAGS\n",
        "m1_prime, m1_prior_tags = elapse_time(m0)\n",
        "print(m1_prime)\n",
        "print(m1_prior_tags)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01300929 0.01230319 0.00834417 0.01870376 0.00654152 0.02120422\n",
            " 0.00157563 0.03494667 0.04146768 0.00677726 0.02840866 0.0164832\n",
            " 0.02241779 0.00253611 0.00291206 0.04178426 0.02093562]\n",
            "[ 5  7  3 10 12  1  6  5 14  3 13 11 16 15 14  9 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifBYfZpruH2l"
      },
      "source": [
        "You can check your implementation above by finding $m_1'$ given $m_0$ where $m_0$ is a uniform distribution. It should approximately look like\n",
        "```\n",
        "[0.01300929, 0.01230319, 0.00834417, 0.01870376, 0.00654152,\n",
        " 0.02120422, 0.00157563, 0.03494667, 0.04146768, 0.00677726,\n",
        " 0.02840866, 0.0164832 , 0.02241779, 0.00253611, 0.00291206,\n",
        " 0.04178426, 0.02093562]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBgLK4Cpj5w"
      },
      "source": [
        "## Coding 3 (5 points)\n",
        "\n",
        "The second step of the Viterbi algorithm is to reweight probabilities according to observations. Given the \"time elapsed\" message distribution ```mprime``` and an ```obs``` (a word), the function below should return an updated distribution weighted by the emission probabilities for this observation. Remember to account for unseen words by using the ```#UNSEEN``` word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovQm2FmpBw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad903c4-c5f7-43f4-b74a-590aaca698ed"
      },
      "source": [
        "def observe(mprime, obs):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  by weighting mprime by the emission probabilities corresponding to obs\n",
        "  \"\"\"\n",
        "  # m = np.zeros(NUM_TAGS)\n",
        "  # YOUR CODE HERE\n",
        "  if not obs in OPROBS:\n",
        "    obs = '#UNSEEN'\n",
        "  m = np.multiply(OPROBS[obs], mprime)\n",
        "  return m\n",
        "\n",
        "m1 = observe(m1_prime, '#UNSEEN')\n",
        "print(m1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.04248647e-07 6.97495062e-08 7.90983910e-08 1.51409439e-07\n",
            " 9.75313200e-08 1.30190262e-07 2.28982743e-07 1.00481822e-07\n",
            " 1.03718457e-06 1.21737628e-07 1.52922985e-07 1.27321753e-07\n",
            " 9.46733027e-08 6.60257072e-08 4.86071760e-07 1.81102981e-07\n",
            " 2.47144591e-06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyn5QcGgvAAh"
      },
      "source": [
        "You can check your implementation above by finding $m_1$ given $m_1'$ (from your ```elapse_time``` test above) and an observation of your choosing. If the observation is an ```#UNSEEN``` word, you should find that $m_1$ looks something like \n",
        "```\n",
        "[1.04248647e-07, 6.97495062e-08, 7.90983910e-08, 1.51409439e-07,\n",
        " 9.75313200e-08, 1.30190262e-07, 2.28982743e-07, 1.00481822e-07,\n",
        " 1.03718457e-06, 1.21737628e-07, 1.52922985e-07, 1.27321753e-07,\n",
        " 9.46733027e-08, 6.60257072e-08, 4.86071760e-07, 1.81102981e-07,\n",
        " 2.47144591e-06]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLewyJPOrhPN"
      },
      "source": [
        "## Coding 4 (20 points)\n",
        "\n",
        "You will now put the two functions you wrote above together into one wrapper ```viterbi``` function. Given a list of word observations, it should return a corresponding list of predicted tags. As you hopefully recall, Viterbi runs in two phases. The \"forward\" phase starts with the ```X0``` distribution and goes through the observations one at a time, modifying the message distribution through time and observation updates for each. You should maintain a growing list of most likely tag pointers that is returned from each elapse time update.\n",
        "\n",
        "The second phase then goes backward. Starting with the tag with the highest likelihood at the end, follow the pointers backward to find the most likely tag for each observation. These tags should then be returned as one overall list of tags when finished (make sure the list of tags is in the right order!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xqPy45-E3hX"
      },
      "source": [
        "def viterbi(observations):\n",
        "  \"\"\"\n",
        "  Given a list of word observations, return a list of predicted tags\n",
        "  \"\"\"\n",
        "  m = np.ones(NUM_TAGS)/NUM_TAGS\n",
        "  # YOUR CODE HERE\n",
        "  hist = []\n",
        "\n",
        "  # \"Forward\" phase of the Viterbi algorithm\n",
        "  for obs in observations:\n",
        "    mprime, prio = elapse_time(m)\n",
        "    hist += prio,\n",
        "    m = observe(mprime, obs)\n",
        "  \n",
        "  # \"Backward\" phase of the Viterbi algorithm\n",
        "  i = np.argmax(m)\n",
        "  ret = [TAGS[i]]\n",
        "  for j in range(len(hist))[::-1]:\n",
        "    i = hist[j][i]\n",
        "    ret += TAGS[i],\n",
        "  \n",
        "  return (ret[::-1])[1:]\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkSnP9YAv5qE"
      },
      "source": [
        "Again, we recommend that you check your implementation with small test cases. For example, running ```viterbi(['#UNSEEN'])``` (or any unseen word in place of ```#UNSEEN```) should return ```['X']```, which means that we are unable to determine the POS tag for a previously unseen word without any context.\n",
        "\n",
        "## Response 2 (5 points)\n",
        "\n",
        "Run your Viterbi implementation on the three phrases below. Explain why the tag prediction is different for the word ```'round'``` in each of them. You can refer to the tag descriptions on UD's [website](https://universaldependencies.org/u/pos/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpCXeCRSxqlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca6ca40-5067-4617-e172-58dfdee70571"
      },
      "source": [
        "print(viterbi(['a', 'round', 'circle']))\n",
        "print(viterbi(['play', 'another', 'round']))\n",
        "print(viterbi(['walk', 'round', 'the', 'fence']))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DET', 'ADJ', 'NOUN']\n",
            "['VERB', 'DET', 'NOUN']\n",
            "['VERB', 'ADP', 'DET', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOdemDNCT7N"
      },
      "source": [
        "ENTER YOUR RESPONSE HERE\n",
        "\n",
        "Since in viterbi, when calculate the probability of each tag, we not only consider the observation, but also consiter the transition matrix. In the first phrase, since the third word is a 'NOUN', so the second word has high probability to be a adjective accoring to the grammar. In the second phrase, since the second word is a 'DET', so the third word has high probability to be a noun accoring to the grammar. In the third phrase, since the fisrt word is a 'VERB', so the second word has high probability to be an adposition accoring to the grammar. In a words, we are considering the context (the words before and after the word we want to predict) now in the Viterbi implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsUtrNFwJai"
      },
      "source": [
        "Once you are confident of your implementation, you can run it on the full training and test data sets. You should find that both accuracy rates are better than those of the unigram model, at about 95\\% for the training set and (more importantly) 88\\% for the test set. What's more, the accuracy rate on unseen words should be about 29\\%, a very significant improvement over the unigram predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R30oE7Q9HsiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32fd69fa-8f2c-4b84-a741-a8e92c35731d"
      },
      "source": [
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'viterbi')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9520788633819958\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8766784874686218\n",
            "Accuracy rate on unseen words:  0.293630017452007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcZCyFORyw0y"
      },
      "source": [
        "Viterbi's performance is held back by its accuracy rate on unseen words. One option is to build a higher-order HMM, e.g. a trigram model with transitions determined by the previous two states rather than one state. \n",
        "\n",
        "An alternative fix is to consider the emission probabilities of the ```#UNSEEN``` tag. As you saw in the second coding cell that learns the parameters, the ```#UNSEEN``` tag gets the same count of ```alpha``` for all POS tags. This is equivalent to saying that in a typical corpus, the distribution of unseen words is uniform across all tags. This is not a great assumption; there are probably many more nouns or verbs that we have not encountered than conjunctions or punctuation marks.\n",
        "\n",
        "Instead of a uniform distribution, it may be more reasonable to assume that the distribution of unseen words is similar to that of words that appear only once in the training set. These words are known as [*hapax legomena*](https://en.wikipedia.org/wiki/Hapax_legomenon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4i0FIt92ol"
      },
      "source": [
        "## Coding 5 (5 points)\n",
        "\n",
        "Complete the ```hapax_POS_counts``` function below. Given a dictionary ```obs_counts```, which maps a word to a numpy array counting the number of times it occurred as each POS tag, identify all words with a total count of 1. Add up the POS tag occurrences for each of these words, and return the total POS tag counts over all hapax legomena words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkXdM-ns2gaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5699422-fc2a-429c-acc6-e6c1a0e3e1ed"
      },
      "source": [
        "def hapax_POS_counts(obs_counts):\n",
        "  \"\"\"\n",
        "  Given a dictionary of word to count arrays, return the total number \n",
        "  of POS tag appearances only for the words with a total count of 1\n",
        "  \"\"\"\n",
        "  hapax = np.zeros(NUM_TAGS)\n",
        "  # YOUR CODE HERE\n",
        "  for obs in obs_counts:\n",
        "    if np.sum(obs_counts[obs]) == 1:\n",
        "      hapax = np.add(hapax, np.array(obs_counts[obs]))\n",
        "  print(np.divide(hapax, np.sum(hapax)))\n",
        "  return hapax\n",
        "\n",
        "hapax = hapax_POS_counts(obs_counts)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.08174304e-01 3.57179304e-03 3.14317788e-02 2.04102459e-03\n",
            " 4.08204919e-04 1.22461476e-03 5.51076640e-03 3.58608021e-01\n",
            " 6.38840698e-02 2.04102459e-04 2.95948566e-03 2.18185529e-01\n",
            " 4.08204919e-03 1.42871722e-03 3.06153689e-03 1.67874273e-01\n",
            " 2.73497296e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y82n2w5cPz8"
      },
      "source": [
        "## Response 3 (5 points)\n",
        "\n",
        "You can check your hapax distribution by verifying that the POS tags of hapax legomena tend to be those associated with a greater diversity of words. Which two POS tags occur most often, and which two POS tags occur least often? Briefly explain, again using some context from the structure of the English language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnoQmowYcSQ0"
      },
      "source": [
        "YOUR RESPONSE HERE\n",
        "\n",
        "Least often: CCONJ, PART\n",
        "Most often: NOUN, PROPN\n",
        "\n",
        "The reason is that in the text, some words appear more frequently, while some others appear less frequently. The words that are CCONJ (coordinating conjunction) or PART (particle) appear more frequently, since they can always be added without considering what topic the text is talking about. So it is very likely that appear more than once in the text, in different sentances. However, the words that are NOUN (noun) or PROPN (proper noun) appear less frequently, and with a great posibility that they appear only once in the text: they only appear in the first several lines, and then we use pronoun to refer to them. This make them highly likely to be a hapax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtIl-uzzAtnM"
      },
      "source": [
        "\n",
        "Once you've successfully produced a distribution of hapax legomena POS tags, run the cell below. ```OPROBS``` will be recomputed using the new distribution, and Viterbi will be run again on the test data set. The accuracy rate on all words should see a slight tick up, but the accuracy on unseen words should see a marked improvement to about 45\\%. This means that we can correctly tag almost half of words that we have never encountered!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0N1ck989SEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2de290-8694-4977-c699-9f42da082d20"
      },
      "source": [
        "hapax = hapax_POS_counts(obs_counts)\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts + hapax)\n",
        "OPROBS['#UNSEEN'] = np.divide(hapax, tag_counts + hapax)\n",
        "\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8938518548033629\n",
            "Accuracy rate on unseen words:  0.4493891797556719\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}